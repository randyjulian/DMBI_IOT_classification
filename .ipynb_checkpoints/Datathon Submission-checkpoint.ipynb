{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b4b1ed634710>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cross_validation'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from pandas.api.types import is_object_dtype\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_columns\",300)\n",
    "pd.set_option(\"display.max_rows\",300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_rank(train,model,n):\n",
    "    cols=train.columns\n",
    "    col_indices = np.argsort(model.feature_importances_)[::-1]\n",
    "    feature_ranking_gb = pd.DataFrame(columns=['indicie', 'variable', 'importance'])\n",
    "    gb_top_col_list = []\n",
    "    for f in range(n): \n",
    "        z = pd.DataFrame([col_indices[f],cols[col_indices[f]],model.feature_importances_[col_indices[f]]]).transpose()\n",
    "        z.columns = ['indicie', 'variable', 'importance']\n",
    "        gb_top_col_list.append(cols[col_indices[f]])\n",
    "        feature_ranking_gb = feature_ranking_gb.append(z)\n",
    "    return feature_ranking_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data included is relatively clean other than for null values, so we are going ot just remove the rows with null values as there are not many of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Copied JX's code to clean df\n",
    "## Need only be run once to clean the data, the csv will be used later\n",
    "df=pd.read_csv('/Users/randyjulian/Downloads/hackathon_IoT_training_set_based_on_01mar2017.csv')\n",
    "\n",
    "# separating independent and dependent variables\n",
    "ind = df.iloc[:, :-1]\n",
    "dep = df.iloc[:, -1]\n",
    "\n",
    "ind = ind.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "df2 = pd.concat([ind, dep], axis=1)\n",
    "no_nans = df2.isnull().sum()\n",
    "no_nans.to_csv('number of nans per column.csv')\n",
    "cleaned_df = df2.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping cleaned df as pickle files\n",
    "joblib.dump(cleaned_df,'cleaned_df.pkl',compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading pickle files for re-use\n",
    "cleaned_df=joblib.load('cleaned_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model (RandomForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "train_init, test_init, train_init_y, test_init_y= train_test_split(cleaned_df.iloc[:,0:-1],cleaned_df.device_category, test_size=0.3,random_state=21374)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building initial/baseline randomforest model\n",
    "rf_init=RandomForestClassifier(n_estimators=50, verbose=10,class_weight='balanced',random_state=21374)\n",
    "rf_init.fit(train_init,train_init_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting top 50 var from RF to csv\n",
    "feature_rank(train_init,rf_init,50).to_csv('top50rfinit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_init.score(test_init,test_init_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Reduction for Highly Correlated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the code is meant to run for binary variable instead of multiclass problems, I ran the correlation reduction using one-vs-rest method. Meaning, I ran the code 10 times:\n",
    "1. For each run, I relabel one device_category as 1 and the rest as 0 (eg. water sensor)\n",
    "2. It gives me a set of reduced variables when water_sensor is classified as 1 and the rest as 0\n",
    "3. The method is repeated for each device_category, find the intersection of all 10 set of reduced variables as the final variables used in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_reduction(corrMatrix, threshold=0.5): \n",
    "    corrmat = abs(corrMatrix.copy(deep=True))\n",
    "\n",
    "    #Correlation Threshold\n",
    "    for col in corrMatrix: \n",
    "        if col in corrMatrix.keys(): \n",
    "            thisCol=[]\n",
    "            thisVars=[]\n",
    "            for i in range(len(corrMatrix)): \n",
    "                if (abs(corrMatrix[col][i])==1.0) and (col != corrMatrix.keys()[i]): \n",
    "                    thisCorr=0\n",
    "                else: \n",
    "                    #tag the highly corelated one as positive and the rest as negative\n",
    "                    thisCorr = (1 if abs(corrMatrix[col][i])>threshold else -1) * abs(target[corrMatrix.keys()[i]].values[0])\n",
    "                thisCol.append(thisCorr)\n",
    "                thisVars.append(corrMatrix.keys()[i])\n",
    "            mask = np.ones(len(thisCol), dtype=bool)\n",
    "            ctDelCol = 0 \n",
    "            for n, j in enumerate(thisCol): \n",
    "                mask[n]=not (j!=max(thisCol) and j >=0)\n",
    "                 # keep the max, remove the rest of highly correlated ones\n",
    "                if j !=max(thisCol) and j>=0: \n",
    "                    corrMatrix.pop('%s' %thisVars[n])\n",
    "                    target.pop('%s' %thisVars[n])\n",
    "                    ctDelCol +=1\n",
    "            corrMatrix=corrMatrix[mask]\n",
    "    corrmat = abs(corrMatrix.copy(deep=True))\n",
    "    return corrmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run from here\n",
    "cleaned_copy=cleaned_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## change the x value accordingly for each class. \n",
    "## It is one-vs-rest problem so the value 1 will be assigned to one class for each model\n",
    "\n",
    "# cleaned_copy.device_category = cleaned_copy.device_category.apply(lambda x: 0 if x != 'water_sensor' else 1) \n",
    "# cleaned_copy.device_category = cleaned_copy.device_category.apply(lambda x: 0 if x != 'thermostat' else 1) \n",
    "# cleaned_copy.device_category = cleaned_copy.device_category.apply(lambda x: 0 if x != 'socket' else 1) \n",
    "# cleaned_copy.device_category = cleaned_copy.device_category.apply(lambda x: 0 if x != 'lights' else 1) \n",
    "# cleaned_copy.device_category = cleaned_copy.device_category.apply(lambda x: 0 if x != 'TV' else 1) \n",
    "# cleaned_copy.device_category = cleaned_copy.device_category.apply(lambda x: 0 if x != 'motion_sensor' else 1) \n",
    "# cleaned_copy.device_category = cleaned_copy.device_category.apply(lambda x: 0 if x != 'security_camera' else 1) \n",
    "# cleaned_copy.device_category = cleaned_copy.device_category.apply(lambda x: 0 if x != 'watch' else 1) \n",
    "#cleaned_copy.device_category = cleaned_copy.device_category.apply(lambda x: 0 if x != 'smoke_detector' else 1) \n",
    "cleaned_copy.device_category = cleaned_copy.device_category.apply(lambda x: 0 if x != 'baby_monitor' else 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "npMatrix = pd.DataFrame(np.corrcoef(cleaned_copy,rowvar=0))\n",
    "npMatrix.columns = cleaned_copy.columns\n",
    "npMatrix.index = cleaned_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## forming the new correlation matrix to be feed into the correlation reduction function\n",
    "corrMatrix = npMatrix.copy(deep=True)\n",
    "target=corrMatrix[['device_category']].T\n",
    "corrMatrix.drop('device_category',axis=1,inplace=True)\n",
    "corrMatrix.drop('device_category',axis=0,inplace=True)\n",
    "\n",
    "corrMatrix.dropna(how='all',axis=[0,1], inplace=True)\n",
    "\n",
    "redcorrmat=correlation_reduction(corrMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redcorrmat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### here I changed the name of the list accordingly\n",
    "# water_sensor_reduced=list(redcorrmat)\n",
    "# thermostat_reduced=list(redcorrmat)\n",
    "# socket_reduced=list(redcorrmat)\n",
    "# lights_reduced=list(redcorrmat)\n",
    "# TV_reduced=list(redcorrmat)\n",
    "# motion_sensor_reduced=list(redcorrmat)\n",
    "# security_camera_reduced=list(redcorrmat)\n",
    "# watch_reduced=list(redcorrmat)\n",
    "# smoke_detector_reduced=list(redcorrmat)\n",
    "baby_monitor_reduced=list(redcorrmat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Finding the intersect of all the list of reduced variables from each class\n",
    "list(set(thermostat_reduced).intersection(socket_reduced,lights_reduced,TV_reduced,baby_monitor_reduced,motion_sensor_reduced,security_camera_reduced,watch_reduced,water_sensor_reduced,smoke_detector_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built 3 models for this competition to compare the accuracy and effectiveness of these models:\n",
    "1. rf_20 is built with the top 20 variables obtained from the baseline model\n",
    "2. rf_37 is built with the 37 variables obtained from the correlation reduction section\n",
    "3. rf_57 is built combining both the top 20 variables and the 37 correlation reduction variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RF model for the top 20 variable only\n",
    "rf_20=RandomForestClassifier(n_estimators=50, verbose=10,class_weight='balanced')\n",
    "rf_20.fit(train_init.loc[:,list(feature_rank(train_init,rf_init,20).variable)],train_init_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RF model for the 37 variable from correlation reduction only\n",
    "rf_37=RandomForestClassifier(n_estimators=50, verbose=10,class_weight='balanced')\n",
    "rf_37.fit(train_init.loc[:,list(set(thermostat_reduced).intersection(socket_reduced,lights_reduced,TV_reduced,baby_monitor_reduced,motion_sensor_reduced,security_camera_reduced,watch_reduced,water_sensor_reduced,smoke_detector_reduced))],train_init_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combined list from top 20 variables (Random Forest) and 37 variables (Correlation Reduction)\n",
    "combined_list=list(set(list(set(thermostat_reduced).intersection(socket_reduced,lights_reduced,TV_reduced,baby_monitor_reduced,motion_sensor_reduced,security_camera_reduced,watch_reduced,water_sensor_reduced,smoke_detector_reduced))).union(list(feature_rank(train_init,rf_init,20).variable)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_57=RandomForestClassifier(n_estimators=50, verbose=10,class_weight='balanced')\n",
    "rf_57.fit(train_init.loc[:,combined_list],train_init_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_57.score(test_init.loc[:,combined_list], test_init_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(test_init_y,rf_57.predict(test_init.loc[:,combined_list])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampled Data (resampled data are labelled as xcv..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original half a million data have very small ratio of unknown devices and due to the time constraint, training a data on 500,000 rows are taking too much time. We resample the data to allow us to have a 80:20 ratio of unknown devices so the model can learn better and also to limit the learning to lesser amount of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample dataset 1\n",
    "xcv1=joblib.load('/Users/randyjulian/Downloads/sample (1).pkl')\n",
    "\n",
    "# Resample dataset 2\n",
    "xcv2=joblib.load('/Users/randyjulian/Downloads/sample2.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "IsolationForest is an algorithm used to detect anomalies by giving each observation a score depending if the algorithm consider the observation as an anomaly.\n",
    "\n",
    "The IsolationForest algorithm is used in this problem to measure how well we recognize a device. The process is explained below:\n",
    "1. We train the IsolationForest to learn each device. Hence we built 10 IsolationForest models, one for each device_category. For each model, we only feed in one particular device (eg: lights) in training.\n",
    "2. So we have 10 models that know really well each of their own device, hence we can use this as a \"recognition\" model eg: feeding a motion_sensor data to a lights model will return us as unknown/0 and feeding a lights data will return us a known/1.\n",
    "3. We ran the observation to all the 10 models to give us a vector of 0,1 (0 for unknown, 1 for known) and we sum this binary values across each observation.\n",
    "4. By theory, for any observations, if they belong to any known device_category, the sum should be greater than 1. Hence, the devices whose sum is 0 will be defined as the unknown device_category.\n",
    "\n",
    "For the code below, since we don't know how unknown devices are identified, we select one device_category as \"unknown\" (eg below is lights) in order for us to validate the results of this ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcv21=xc2[xc2.device_category!= 'lights']\n",
    "xcv22=xc2[xc2.device_category== 'lights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I removed lights here to be the unknown class\n",
    "columns=['security_camera','TV','smoke_detector','thermostat','water_sensor','watch','baby_monitor','motion_sensor','socket']\n",
    "top15rf=list(feature_rank(train_init,rf_init,15).variable)\n",
    "\n",
    "def check_unknown(columns,dftrain,dftest,top15var):\n",
    "    df_final=pd.DataFrame()\n",
    "    model_list=[]\n",
    "    n=0\n",
    "    for device in columns:\n",
    "        dfunknown=dftrain[dftrain.device_category== device]\n",
    "        iso=IsolationForest(n_estimators=300, contamination=0.1)\n",
    "        iso.fit(dfunknown.loc[:,top15var])\n",
    "        df_results= pd.DataFrame(iso.predict(dftest.loc[:,top15var]))\n",
    "        df_final[device] = iso.predict(dftest.loc[:,top15var])\n",
    "    df_final['sum_iso']=df_final.apply(sum,axis=1)\n",
    "    #### This line need to be removed for submission, but it need to be included for validation to measure accuracy\n",
    "    #df_final['class']=dftest.device_category.reset_index().device_category\n",
    "    identification_list = []\n",
    "    for entry in range(len(df_final)):\n",
    "        data= df_final.iloc[entry]\n",
    "        if data['sum_iso'] == -len(columns):\n",
    "            identity = \"unknown\"\n",
    "        else:\n",
    "            identity = \"known\"\n",
    "        identification_list.append(identity)\n",
    "    df_identification = pd.DataFrame(identification_list,columns=['identity'])\n",
    "    df_final = pd.merge(df_final,df_identification,left_index=True, right_index=True)\n",
    "    return df_final    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=check_unknown(columns, xcv1, xcv2, top15rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to create the validation data to check accuracy of the isolationForest\n",
    "df_test['true_value']= df_test.apply((lambda x: 'unknown' if x[10]== 'lights' else 'known'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix to see the accuracy of the model\n",
    "first_model_cm=metrics.confusion_matrix(df_test.true_value,df_test.identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_torun=df_test[df_test.identity=='known']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_20.score(test_init.loc[:,list(feature_rank(train_init,rf_init,20).variable)],test_init_y))\n",
    "print(rf_37.score(test_init.loc[:,list(set(thermostat_reduced).intersection(socket_reduced,lights_reduced,TV_reduced,baby_monitor_reduced,motion_sensor_reduced,security_camera_reduced,watch_reduced,water_sensor_reduced,smoke_detector_reduced))],test_init_y))\n",
    "print(rf_57.score(test_init.loc[:,combined_list],test_init_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(rf_57,'rf_57.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_torun.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_list=df_torun.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xctrain1=xcv2.iloc[known_list,:]\n",
    "xctrain2=xctrain1.loc[:,combined_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list=rf_57.predict(xctrain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(df_torun['class'][df_torun.identity=='known'],result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest Iter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcv3=joblib.load('sample3.pkl')\n",
    "xcv4=joblib.load('sample4.pkl')\n",
    "xcv5=joblib.load('sample5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xc34=pd.concat([xcv3,xcv4], axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1=check_unknown(columns, xc34, xcv5, top10rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1['true_value']= df_test1.apply((lambda x: 'unknown' if x[10]== 'lights' else 'known'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_model_cm=metrics.confusion_matrix(df_test1.true_value,df_test1.identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_model_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(df_test1.true_value,df_test1.identity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(df_test.true_value,df_test.identity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation=pd.read_csv('/Users/randyjulian/Downloads/hackathon_IoT_validation_set_based_on_01mar2017_ANONYMIZED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.identity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df=check_unknown(list(cleaned_df.device_category.unique()),xcv1,validation,top15rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df2=check_unknown(list(cleaned_df.device_category.unique()),xc34,validation,top15rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_score(submission_df, validation_df,model,xgboost_indicator):\n",
    "    known_df=submission_df[submission_df.identity=='known']\n",
    "    known_list=known_df.index.tolist()\n",
    "    \n",
    "    train1=validation_df.iloc[known_list,:]\n",
    "    train2=train1.loc[:,combined_list]\n",
    "    if xgboost_indicator == False:\n",
    "        result_list=model.predict(train2)\n",
    "        return list(result_list),known_list\n",
    "    else:\n",
    "        dval = xgb.DMatrix(data=train2)\n",
    "        xgb_pred = model.predict(dval)\n",
    "        return list(xgb_pred), known_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list,known_list=model_score(submission_df,validation,rf_57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list_37,known_list_37=model_score(submission_df,validation,rf_37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_score(submission_df,result_list,known_list):\n",
    "    identity_list = []\n",
    "    position = 0 \n",
    "    for entry in range(len(submission_df)):\n",
    "        data=submission_df.iloc[entry]\n",
    "        if entry in known_list:\n",
    "            identity = result_list[position]\n",
    "            position +=1\n",
    "        else:\n",
    "            identity = \"unknown\"\n",
    "        identity_list.append(identity)\n",
    "    \n",
    "    df_identity = pd.DataFrame(identity_list,columns=[\"final_identity\"])\n",
    "    df_final = pd.merge(submission_df, df_identity,left_index=True, right_index=True)\n",
    "    return df_final\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_submission = cat_score(submission_df,result_list,known_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alan1 = df_final_submission.final_identity.to_frame()\n",
    "df_alan2 = validation.session_ind.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_confirm = pd.merge(df_alan1,df_alan2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_confirm.columns = [\"device_category\", \"session_ind\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_confirm.set_index('device_category',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_confirm.to_csv('submission_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_submission_37 = cat_score(submission_df,result_list_37,known_list_37)\n",
    "df_alan1_37 = df_final_submission_37.final_identity.to_frame()\n",
    "df_alan2_37 = validation.session_ind.to_frame()\n",
    "df_submission_confirm_37 = pd.merge(df_alan1_37,df_alan2_37, left_index=True, right_index=True)\n",
    "df_submission_confirm_37.columns = [\"device_category\", \"session_ind\"]\n",
    "df_submission_confirm_37.set_index('device_category',inplace=True)\n",
    "df_submission_confirm_37.to_csv('submission_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_1=joblib.load('xgb_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2=validation.loc[:,combined_list]\n",
    "dval = xgb.DMatrix(data=train2)\n",
    "xgb_pred = xgb_1.predict(dval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list_xgb,known_list_xgb=model_score(submission_df,validation,xgb_1,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_id=joblib.load('encoded_id.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list_xgb_final = list(np.array(result_list_xgb) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_list_2 = []\n",
    "for i in result_list_xgb_final:\n",
    "    identity = encoded_id[i]\n",
    "    identity_list_2.append(identity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_submission_xgb = cat_score(submission_df,identity_list_2,known_list_xgb)\n",
    "df_alan1_xgb = df_final_submission_xgb.final_identity.to_frame()\n",
    "df_alan2_xgb = validation.session_ind.to_frame()\n",
    "df_submission_confirm_xgb = pd.merge(df_alan1_xgb,df_alan2_xgb, left_index=True, right_index=True)\n",
    "df_submission_confirm_xgb.columns = [\"device_category\", \"session_ind\"]\n",
    "df_submission_confirm_xgb.set_index('device_category',inplace=True)\n",
    "df_submission_confirm_xgb.to_csv('submission_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### XBG with Iso2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list_xgb2,known_list_xgb2=model_score(submission_df2,validation,xgb_1,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list_xgb_final2 = list(np.array(result_list_xgb2) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_list_22 = []\n",
    "for i in result_list_xgb_final2:\n",
    "    identity = encoded_id[i]\n",
    "    identity_list_22.append(identity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_submission_xgb2 = cat_score(submission_df2,identity_list_22,known_list_xgb2)\n",
    "df_alan1_xgb2 = df_final_submission_xgb2.final_identity.to_frame()\n",
    "df_alan2_xgb2 = validation.session_ind.to_frame()\n",
    "df_submission_confirm_xgb2 = pd.merge(df_alan1_xgb2,df_alan2_xgb2, left_index=True, right_index=True)\n",
    "df_submission_confirm_xgb2.columns = [\"device_category\", \"session_ind\"]\n",
    "df_submission_confirm_xgb2.set_index('device_category',inplace=True)\n",
    "df_submission_confirm_xgb2.to_csv('submission_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df2.identity.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

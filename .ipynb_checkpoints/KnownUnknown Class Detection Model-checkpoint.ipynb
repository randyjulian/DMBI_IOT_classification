{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report, f1_score, roc_auc_score, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer, StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras import Sequential\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dropout, Dense\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load cleaned data from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = joblib.load('pickles/cleaned_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load top 50 features and extract top 50 features into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_feats_list = joblib.load('pickles/57var.pkl')\n",
    "trimmed_feats_list.append('device_category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter dataset to only use the 57 relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = cleaned_df.filter(items=trimmed_feats_list, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(399348, 57)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain list of unique classes and create list of names of feature engineered columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_classes = filtered_df.device_category.unique()\n",
    "training_cols = ['sd', 'sem', 'var', 'skew', 'kurt', 'mad', 'shortest_dist_sd', 'sd_diff_smallest2next', 'known']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build training dataset for known/unknown detection classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_df = pd.DataFrame()\n",
    "knn = KNeighborsClassifier(n_neighbors=9, weights='uniform', algorithm='auto', p=2, n_jobs=-1, leaf_size=30)\n",
    "\n",
    "for out_class in distinct_classes:\n",
    "    nine_classes_list = distinct_classes.tolist()\n",
    "    nine_classes_list.remove(out_class)\n",
    "    \n",
    "    in_classes_samples = pd.DataFrame()\n",
    "    nine_classes_df = filtered_df[filtered_df['device_category'] != out_class]\n",
    "    out_class_df = filtered_df[filtered_df['device_category'] == out_class]\n",
    "    \n",
    "    for in_class in nine_classes_list:\n",
    "        in_class_df = filtered_df[filtered_df['device_category'] == in_class]\n",
    "        in_class_sample = in_class_df.sample(50)\n",
    "        index_to_drop = in_class_sample.index.tolist()\n",
    "        nine_classes_df = nine_classes_df.drop(index_to_drop)\n",
    "        in_class_sample = in_class_sample.reset_index()\n",
    "        in_classes_samples = pd.concat([in_classes_samples, in_class_sample], axis=0, ignore_index=True)\n",
    "    \n",
    "    train_x, _, train_y, _ = train_test_split(nine_classes_df.iloc[:, :-1], nine_classes_df.iloc[:, -1], train_size=900, \n",
    "                                              test_size=9, random_state=1, shuffle=True, stratify=nine_classes_df.iloc[:, -1])\n",
    "    knn.fit(train_x, train_y)\n",
    "    \n",
    "    val_x, val_y = in_classes_samples.iloc[:, 1:-1], in_classes_samples.iloc[:, -1]\n",
    "    val_output = knn.kneighbors(val_x, n_neighbors=9, return_distance=True)\n",
    "    val_distance_df = pd.DataFrame([[distance for distance in row] for row in val_output[0]])\n",
    "    val_sd = val_distance_df.std(axis=1, numeric_only=True)\n",
    "    val_min = val_distance_df.min(axis=1, numeric_only=True)\n",
    "    val_mean = val_distance_df.mean(axis=1, numeric_only=True)\n",
    "    val_2nd_smallest_dist_df = pd.DataFrame(np.sort(val_distance_df.values))\n",
    "    val_2nd_smallest_dist_df = val_2nd_smallest_dist_df.iloc[:, -2]\n",
    "    \n",
    "    val_distance_df['sd'] = val_sd\n",
    "    val_distance_df['sem'] = val_distance_df.sem(axis=1, numeric_only=True)\n",
    "    val_distance_df['var'] = val_distance_df.var(axis=1, numeric_only=True)\n",
    "    val_distance_df['skew'] = val_distance_df.skew(axis=1, numeric_only=True)\n",
    "    val_distance_df['kurt'] = val_distance_df.kurt(axis=1, numeric_only=True)\n",
    "    val_distance_df['mad'] = val_distance_df.mad(axis=1)\n",
    "    val_distance_df['shortest_dist_sd'] = (val_mean - val_min) / val_sd\n",
    "    val_distance_df['sd_diff_smallest2next'] = (val_2nd_smallest_dist_df - val_min) / val_sd \n",
    "    val_distance_df['known'] = pd.Series([1 for _ in range(len(val_distance_df.index))]) \n",
    "    \n",
    "    \n",
    "    out_class_df = out_class_df.sample(n=450, random_state=1, axis=0, replace=True)\n",
    "    test_x, test_y = out_class_df.iloc[:, :-1], out_class_df.iloc[:, -1]\n",
    "    test_output = knn.kneighbors(test_x, n_neighbors=9, return_distance=True)\n",
    "    \n",
    "    test_distance_df = pd.DataFrame([[distance for distance in row] for row in test_output[0]])\n",
    "    test_sd = test_distance_df.std(axis=1, numeric_only=True)\n",
    "    test_min = test_distance_df.min(axis=1, numeric_only=True)\n",
    "    test_mean = test_distance_df.mean(axis=1, numeric_only=True)\n",
    "    test_2nd_smallest_dist_df = pd.DataFrame(np.sort(test_distance_df.values))\n",
    "    test_2nd_smallest_dist_df = test_2nd_smallest_dist_df.iloc[:, -2]\n",
    "    \n",
    "    test_distance_df['sd'] = test_sd\n",
    "    test_distance_df['sem'] = test_distance_df.sem(axis=1, numeric_only=True)\n",
    "    test_distance_df['var'] = test_distance_df.var(axis=1, numeric_only=True)\n",
    "    test_distance_df['skew'] = test_distance_df.skew(axis=1, numeric_only=True)\n",
    "    test_distance_df['kurt'] = test_distance_df.kurt(axis=1, numeric_only=True)\n",
    "    test_distance_df['mad'] = test_distance_df.mad(axis=1)\n",
    "    test_distance_df['shortest_dist_sd'] = (test_mean - test_min) / test_sd\n",
    "    test_distance_df['sd_diff_smallest2next'] = (test_2nd_smallest_dist_df - test_min) / test_sd \n",
    "    test_distance_df['known'] = pd.Series([0 for _ in range(len(test_distance_df.index))]) \n",
    "    \n",
    "    \n",
    "    combined_df = pd.concat([val_distance_df, test_distance_df], axis=0, ignore_index=True).filter(items=training_cols, axis='columns')\n",
    "    training_df = pd.concat([training_df, combined_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distribution of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_df['known'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = training_df.replace([np.inf, -np.inf], np.nan)\n",
    "training_df = training_df.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the labelled dataset obtained from KNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(training_df, 'pickles/known_classifier_dataset_v2.pkl', compress=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the labelled dataset obtained from KNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = joblib.load('pickles/known_classifier_dataset_v2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare feature engineered dataset, splitting to train-test using 80-20 ratio\n",
    "Scale using standard scaling so that models are able to better learn from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = training_df.drop('known', axis=1), training_df['known']\n",
    "class_weights = compute_class_weight('balanced', np.unique(y), y)\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model for KnownUnknown Class Detection\n",
    "#### Using Stochastic Average Gradient for negate the large dataset impact on training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 123 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=2000,\n",
       "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=5,\n",
       "          solver='sag', tol=0.0001, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(penalty='l2', dual=False, class_weight='balanced', random_state=5, max_iter=2000, \n",
    "                             multi_class='ovr', verbose=1, n_jobs=-1, solver='sag')\n",
    "log_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_preds = log_reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Report to validate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.67      0.61       894\n",
      "          1       0.60      0.49      0.54       906\n",
      "\n",
      "avg / total       0.58      0.58      0.57      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, log_reg_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model for KnownUnknown Class Detection\n",
    "#### Using 1000 Decision Trees, Bagging and balancing class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    6.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=1000, n_jobs=-1, oob_score=True, random_state=5,\n",
       "            verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(1000, criterion='gini', oob_score=True, n_jobs=-1,\n",
    "                            random_state=5, class_weight='balanced', verbose=1)\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:    1.4s finished\n"
     ]
    }
   ],
   "source": [
    "rf_preds = rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.74      0.81       894\n",
      "          1       0.78      0.91      0.84       906\n",
      "\n",
      "avg / total       0.83      0.82      0.82      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, rf_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model for KnownUnknown Class Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(x_train, label=np.array(y_train))\n",
    "dtest = xgb.DMatrix(x_test, label=np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth': 10,\n",
    "    'eta': 0.1,\n",
    "    'silent': 0,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'error'\n",
    "}\n",
    "num_round = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.17625+0.00233854\ttest-error:0.238056+0.00540402\n",
      "[1]\ttrain-error:0.174583+0.00172609\ttest-error:0.237639+0.00510673\n",
      "[2]\ttrain-error:0.175139+0.00215375\ttest-error:0.2375+0.00450057\n",
      "[3]\ttrain-error:0.174791+0.00230112\ttest-error:0.238195+0.00501556\n",
      "[4]\ttrain-error:0.174791+0.00206239\ttest-error:0.238472+0.00538285\n",
      "[5]\ttrain-error:0.175+0.0037071\ttest-error:0.236945+0.00665207\n",
      "[6]\ttrain-error:0.174653+0.00398666\ttest-error:0.236945+0.00665207\n",
      "[7]\ttrain-error:0.173194+0.00202971\ttest-error:0.235694+0.00594482\n",
      "[8]\ttrain-error:0.171389+0.00475163\ttest-error:0.23+0.0116268\n",
      "[9]\ttrain-error:0.165486+0.00413993\ttest-error:0.225833+0.0121385\n",
      "[10]\ttrain-error:0.165139+0.00478827\ttest-error:0.225139+0.0124737\n",
      "[11]\ttrain-error:0.164792+0.00417041\ttest-error:0.22375+0.0124164\n",
      "[12]\ttrain-error:0.165694+0.00383521\ttest-error:0.225694+0.0126669\n",
      "[13]\ttrain-error:0.16375+0.00478997\ttest-error:0.223472+0.0120251\n",
      "[14]\ttrain-error:0.162778+0.00587359\ttest-error:0.223195+0.0122538\n",
      "[15]\ttrain-error:0.162083+0.00641702\ttest-error:0.221806+0.0100363\n",
      "[16]\ttrain-error:0.162639+0.00634728\ttest-error:0.222361+0.0104599\n",
      "[17]\ttrain-error:0.160486+0.00631308\ttest-error:0.220972+0.0112287\n",
      "[18]\ttrain-error:0.161111+0.00633097\ttest-error:0.220695+0.0107602\n",
      "[19]\ttrain-error:0.159375+0.00634902\ttest-error:0.219306+0.0125567\n",
      "[20]\ttrain-error:0.159583+0.0062941\ttest-error:0.218889+0.0121256\n",
      "[21]\ttrain-error:0.159305+0.0062135\ttest-error:0.218472+0.0117963\n",
      "[22]\ttrain-error:0.159028+0.0060386\ttest-error:0.219305+0.0111405\n",
      "[23]\ttrain-error:0.156667+0.00468004\ttest-error:0.21875+0.0117011\n",
      "[24]\ttrain-error:0.156528+0.00453049\ttest-error:0.218194+0.0115183\n",
      "[25]\ttrain-error:0.157361+0.003732\ttest-error:0.219861+0.0110675\n",
      "[26]\ttrain-error:0.157292+0.00456753\ttest-error:0.220555+0.0100595\n",
      "[27]\ttrain-error:0.15875+0.00325846\ttest-error:0.219167+0.00902038\n",
      "[28]\ttrain-error:0.157153+0.00502994\ttest-error:0.218333+0.00882599\n",
      "[29]\ttrain-error:0.155903+0.00363781\ttest-error:0.217778+0.00916862\n",
      "[30]\ttrain-error:0.156389+0.00270201\ttest-error:0.218056+0.00833537\n",
      "[31]\ttrain-error:0.155+0.00335039\ttest-error:0.216528+0.00970833\n",
      "[32]\ttrain-error:0.156181+0.00228419\ttest-error:0.2175+0.00918559\n",
      "[33]\ttrain-error:0.15507+0.00321086\ttest-error:0.216389+0.00965477\n",
      "[34]\ttrain-error:0.154653+0.00353679\ttest-error:0.216528+0.0102984\n",
      "[35]\ttrain-error:0.154792+0.00280026\ttest-error:0.216389+0.00977963\n",
      "[36]\ttrain-error:0.154375+0.00286132\ttest-error:0.216389+0.00987379\n",
      "[37]\ttrain-error:0.154305+0.00244336\ttest-error:0.215972+0.0103264\n",
      "[38]\ttrain-error:0.153889+0.0025251\ttest-error:0.214306+0.00733104\n",
      "[39]\ttrain-error:0.152569+0.000519645\ttest-error:0.214445+0.00748699\n",
      "[40]\ttrain-error:0.151528+0.000687611\ttest-error:0.215417+0.00858617\n",
      "[41]\ttrain-error:0.152014+0.00109373\ttest-error:0.215+0.00892371\n",
      "[42]\ttrain-error:0.152222+0.00231589\ttest-error:0.215694+0.00783475\n",
      "[43]\ttrain-error:0.152014+0.00186593\ttest-error:0.215556+0.00794476\n",
      "[44]\ttrain-error:0.151528+0.00254788\ttest-error:0.215+0.00784703\n",
      "[45]\ttrain-error:0.152083+0.00245353\ttest-error:0.215278+0.00831476\n",
      "[46]\ttrain-error:0.151458+0.00223099\ttest-error:0.215+0.00858633\n",
      "[47]\ttrain-error:0.151181+0.00260409\ttest-error:0.214583+0.0085867\n",
      "[48]\ttrain-error:0.15125+0.00286132\ttest-error:0.215139+0.00827987\n",
      "[49]\ttrain-error:0.150903+0.00335647\ttest-error:0.215417+0.00800014\n"
     ]
    }
   ],
   "source": [
    "cv = xgb.cv(param, dtrain, num_round, folds=5, stratified=True, metrics='error', verbose_eval=1, seed=42, shuffle=True)\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(param, dtrain, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_prob_preds = bst.predict(dtest)\n",
    "xgb_preds = np.asarray([1 if line > 0.5 else 0 for line in xgb_prob_preds]).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.74      0.81       894\n",
      "          1       0.78      0.91      0.84       906\n",
      "\n",
      "avg / total       0.83      0.82      0.82      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, rf_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model for KnownUnknown Class Detection (3 Hidden Layers)\n",
    "#### Using relu as activation functions for hidden layers due to its robustness and the absence of vanishing gradient problem when using it. Sigmoid is used as the activation function for the output as it is a binary classification problem.\n",
    "\n",
    "#### Loss function of binary crossentropy is used as it is a binary problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'known_classifier_nn.h5'\n",
    "\n",
    "nn_model = Sequential()\n",
    "\n",
    "nn_model.add(Dense(500, activation='relu', input_shape=tuple(x.shape[1:])))\n",
    "nn_model.add(Dropout(0.3))\n",
    "nn_model.add(Dense(1000, activation='relu'))\n",
    "nn_model.add(Dropout(0.3))\n",
    "nn_model.add(Dense(500, activation='relu'))\n",
    "nn_model.add(Dropout(0.3))\n",
    "# nn_model.add(Dense(400, activation='relu'))\n",
    "# nn_model.add(Dropout(0.2))\n",
    "# nn_model.add(Dense(400, activation='relu'))\n",
    "# nn_model.add(Dropout(0.2))\n",
    "nn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "op = optimizers.Adam(lr=0.001)\n",
    "\n",
    "nn_model.compile(optimizer=op, metrics=['accuracy'], loss='binary_crossentropy')\n",
    "\n",
    "save_checkpoint = ModelCheckpoint(model_name, save_best_only=True, verbose=1)\n",
    "early_stop = EarlyStopping(min_delta=0.01, patience=500, verbose=1, mode='min')\n",
    "\n",
    "nn_model.fit(x_train, y_train, epochs=50000, batch_size=16, verbose=2, \n",
    "             class_weight=class_weights, shuffle=True, validation_split=0.2, callbacks=[save_checkpoint, early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_preds = nn_model.predict(x_test)\n",
    "nn_preds = np.array([1 if x > 0.5 else 0 for x in nn_preds]).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.70      0.73       894\n",
      "          1       0.72      0.78      0.75       906\n",
      "\n",
      "avg / total       0.74      0.74      0.74      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, nn_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy scores for all 4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model F1 Score and AUC: 0.576667, 0.535932, 0.577278\n",
      "Random Forest Model F1 Score and AUC: 0.822778, 0.837494, 0.822211\n",
      "XGBoost Model F1 Score and AUC: 0.814444, 0.823467, 0.814140\n",
      "Neural Network Model F1 Score and AUC: 0.739444, 0.750665, 0.739177\n"
     ]
    }
   ],
   "source": [
    "y_test = np.array(y_test)\n",
    "\n",
    "print('Logistic Regression Model F1 Score and AUC: %f, %f, %f' % (accuracy_score(y_test, log_reg_preds), f1_score(y_test, log_reg_preds), roc_auc_score(y_test, log_reg_preds)))\n",
    "print('Random Forest Model F1 Score and AUC: %f, %f, %f' % (accuracy_score(y_test, rf_preds), f1_score(y_test, rf_preds), roc_auc_score(y_test, rf_preds)))\n",
    "print('XGBoost Model F1 Score and AUC: %f, %f, %f' % (accuracy_score(y_test, xgb_preds), f1_score(y_test, xgb_preds), roc_auc_score(y_test, xgb_preds)))\n",
    "print('Neural Network Model F1 Score and AUC: %f, %f, %f' % (accuracy_score(y_test, nn_preds), f1_score(y_test, nn_preds), roc_auc_score(y_test, nn_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on test set using best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading submission dataset and filter to 57 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = pd.read_csv('data/hackathon_IoT_validation_set_based_on_01mar2017_ANONYMIZED.csv')\n",
    "submission_data_filtered = submission_data.filter(items=trimmed_feats_list, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training KNN Model first to obtain distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_x, knn_y = filtered_df.iloc[:, :-1].sample(10000, filtered_df.iloc[:, -1]\n",
    "real_knn = KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='auto', p=2, n_jobs=-1, leaf_size=20)\n",
    "real_knn.fit(knn_x, knn_y)\n",
    "sub_output = real_knn.kneighbors(submission_data_filtered, n_neighbors=10, return_distance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining engineered features from submission dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_distance_df = pd.DataFrame([[distance for distance in row] for row in sub_output[0]])\n",
    "sub_sd = sub_distance_df.std(axis=1, numeric_only=True)\n",
    "sub_min = sub_distance_df.min(axis=1, numeric_only=True)\n",
    "sub_mean = sub_distance_df.mean(axis=1, numeric_only=True)\n",
    "sub_2nd_smallest_dist_df = pd.DataFrame(np.sort(sub_distance_df.values))\n",
    "sub_2nd_smallest_dist_df = sub_2nd_smallest_dist_df.iloc[:, -2]\n",
    "\n",
    "sub_distance_df['sd'] = sub_sd\n",
    "sub_distance_df['sem'] = sub_distance_df.sem(axis=1, numeric_only=True)\n",
    "sub_distance_df['var'] = sub_distance_df.var(axis=1, numeric_only=True)\n",
    "sub_distance_df['skew'] = sub_distance_df.skew(axis=1, numeric_only=True)\n",
    "sub_distance_df['kurt'] = sub_distance_df.kurt(axis=1, numeric_only=True)\n",
    "sub_distance_df['mad'] = sub_distance_df.mad(axis=1)\n",
    "sub_distance_df['shortest_dist_sd'] = (sub_mean - sub_min) / sub_sd\n",
    "sub_distance_df['sd_diff_smallest2next'] = (sub_2nd_smallest_dist_df - sub_min) / sub_sd \n",
    "sub_distance_df['known'] = pd.Series([0 for _ in range(len(sub_distance_df.index))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_sub_x = sub_distance_df.filter(items=training_cols, axis='columns')\n",
    "known_preds =  rf_model.predict(known_sub_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_preds_list = known_preds.tolist()\n",
    "formatted_preds = pd.Series(['unknown' if row == 0 else None for row in known_preds_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_data_filtered = pd.concat([submission_data_filtered, formatted_preds], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobllib.dump(model1_data_filtered, 'unknown_classified_data.pkl', compress=9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
